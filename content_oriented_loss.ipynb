{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import json\n",
    "import math\n",
    "import PIL\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Dataloader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentOrientedDataset(Dataset):\n",
    "    def __init__(self, root='', crop_size=256, \n",
    "        normalize=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = root \n",
    "        img_extensions = ['.jpg', '.png']\n",
    "        self.imgs = []\n",
    "        for ext in img_extensions:\n",
    "            self.imgs += glob.glob(os.path.join(self.data_dir, f'images/**/*{ext}'), recursive=True)\n",
    "        self.crop_size = crop_size\n",
    "        self.image_dims = (3, self.crop_size, self.crop_size)\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        json_file_path = os.path.join(self.data_dir, \"face_coords.json\")\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            self.face_coords = json.load(json_file)\n",
    "\n",
    "    def _augment(self, img, face_masks, structure_masks):\n",
    "        \"\"\"\n",
    "        Apply augmentations \n",
    "        \"\"\"\n",
    "        SCALE_MIN = 0.75\n",
    "        SCALE_MAX = 0.95\n",
    "        H, W, _ = img.shape # slightly confusing\n",
    "        shortest_side_length = min(H,W)\n",
    "        minimum_scale_factor = float(self.crop_size) / float(shortest_side_length)\n",
    "        scale_low = max(minimum_scale_factor, SCALE_MIN)\n",
    "        scale_high = max(scale_low, SCALE_MAX)\n",
    "        scale = np.random.uniform(scale_low, scale_high)\n",
    "\n",
    "        self.augmentations = iaa.Sequential([iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "                                             iaa.Resize((math.ceil(scale * H), math.ceil(scale * W))), # resize\n",
    "                                             iaa.size.CropToFixedSize(self.crop_size,self.crop_size)])\n",
    "        \n",
    "        masks = np.dstack( [face_masks, structure_masks])\n",
    "        masks = SegmentationMapsOnImage(masks, shape=(H,W,2))\n",
    "        img, masks = self.augmentations(image=img, segmentation_maps=masks)\n",
    "        masks = masks.get_arr()\n",
    "        face_masks, structure_masks = masks[:,:,0], masks[:,:,1]\n",
    "        \n",
    "        return img, face_masks, structure_masks\n",
    "\n",
    "    def _transforms(self, img, face_mask, structure_mask):\n",
    "        \"\"\"\n",
    "        Create and apply transforms\n",
    "        \"\"\"\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        transforms_list = [to_tensor]\n",
    "        if self.normalize is True:\n",
    "            transforms_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "        img = transforms.Compose(transforms_list)(img)\n",
    "\n",
    "        face_mask, structure_mask = to_tensor(face_mask).type(torch.int8), to_tensor(structure_mask).type(torch.int8)\n",
    "    \n",
    "        return img, face_mask, structure_mask\n",
    "\n",
    "    def get_face_mask(self, idx, shape): \n",
    "        \"\"\"\n",
    "        Create the face mask \n",
    "        \"\"\"\n",
    "        H,W,_ = shape\n",
    "        mask = np.zeros((H, W, 1), dtype=np.int32)\n",
    "        coords = self.face_coords[os.path.relpath(self.imgs[idx], os.path.join(self.data_dir,\"images\"))]\n",
    "        for coord in coords:\n",
    "             mask[coord[1]:coord[3],coord[0]:coord[2]] = 1 \n",
    "        return mask \n",
    "\n",
    "    def get_structure_mask(self, idx):\n",
    "        \"\"\"\n",
    "        Create the structure mask\n",
    "        \"\"\"\n",
    "        rel_path = os.path.relpath(self.imgs[idx], os.path.join(self.data_dir,\"images\"))\n",
    "        rel_path = os.path.splitext(rel_path)[0]+\".png\"\n",
    "        mask = PIL.Image.open(os.path.join(self.data_dir, \"structure_masks\", rel_path))\n",
    "        mask = np.expand_dims(np.array(mask), axis=-1)\n",
    "        mask = mask / 255 \n",
    "        mask = mask.astype(np.int8)\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        filesize = os.path.getsize(img_path)\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = img.convert('RGB') \n",
    "        img = np.array(img)\n",
    "        H, W, _ = img.shape # slightly confusing\n",
    "        bpp = filesize * 8. / (H * W)\n",
    "        face_mask = self.get_face_mask(idx, img.shape)\n",
    "        structure_mask = self.get_structure_mask(idx)\n",
    "        img, face_mask, structure_mask = self._augment(img, face_mask, structure_mask)    \n",
    "        img, face_mask, structure_mask = self._transforms(img, face_mask, structure_mask)\n",
    "        return [img, face_mask, structure_mask], bpp\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaplacianLoss, self).__init__()\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        if img1.shape[1] == 3:\n",
    "            img1 = TF.rgb_to_grayscale(img1)\n",
    "        if img2.shape[1] == 3:\n",
    "            img2 = TF.rgb_to_grayscale(img2)\n",
    "        laplacian1 = self.laplacian(img1)\n",
    "        laplacian2 = self.laplacian(img2)\n",
    "        laplacian_loss = F.mse_loss(laplacian1, laplacian2, reduction=\"none\")\n",
    "        return laplacian_loss\n",
    "\n",
    "    def laplacian(self, img):\n",
    "        laplacian_kernel = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=torch.float32, device=img.device).unsqueeze(0).unsqueeze(0)\n",
    "        img = F.pad(img, (1, 1, 1, 1), mode='reflect')\n",
    "        laplacian = F.conv2d(img, laplacian_kernel)\n",
    "        return laplacian\n",
    "    \n",
    "class ContentOrientedLoss(nn.Module):\n",
    "    def __init__(self, args, discriminator):\n",
    "        super(ContentOrientedLoss, self).__init__()\n",
    "        self.args = args\n",
    "        self.vgg_perceptual_loss = None\n",
    "        self.laplacian = LaplacianLoss()\n",
    "        self.Discriminator = discriminator\n",
    "\n",
    "    def forward(self, orig_imgs, recon_imgs, face_masks, structure_masks):\n",
    "        weighted_rate = self.rate_loss()\n",
    "        if self.args.normalize_input_image is True:\n",
    "            # [-1.,1.] -> [0.,1.]\n",
    "            orig_imgs = (orig_imgs * 0.5) + 0.5 \n",
    "            recon_imgs = (recon_imgs * 0.5) + 0.5\n",
    "        content_oiented_loss = self.content_oriented_loss(orig_imgs, recon_imgs, face_masks, structure_masks)\n",
    "        rate_content_oriented_loss = weighted_rate + content_oiented_loss\n",
    "        # compute discriminator loss\n",
    "        face_masks, structure_masks, texture_masks = self.process_masks(face_masks, structure_masks)\n",
    "        replaced_recon_imgs = self.real_value_replacement(orig_imgs, recon_imgs, texture_masks)\n",
    "        D_loss = self.gan_loss(orig_imgs, replaced_recon_imgs, \"discriminator_loss\") \n",
    "        return rate_content_oriented_loss, D_loss\n",
    "\n",
    "    def rate_loss(self):\n",
    "        return torch.zeros((0,))\n",
    "\n",
    "    def content_oriented_loss(self, original_img, reconstructed_img, face_mask, structure_mask):\n",
    "        face_mask, structure_mask, texture_mask = self.process_masks(face_mask, structure_mask)\n",
    "        L_tex = self.loss_texture(original_img, reconstructed_img, texture_mask)\n",
    "        L_struc = self.loss_structure(original_img, reconstructed_img, texture_mask)\n",
    "        L_face = self.loss_face(original_img, reconstructed_img, face_mask)\n",
    "        w_L_tex = L_tex\n",
    "        w_L_struc = L_struc * self.args.epsilon\n",
    "        w_L_face = L_face * self.args.gamma\n",
    "        return  L_tex + L_struc + L_face\n",
    "\n",
    "    def loss_face (self, original_img, reconstructed_img, face_mask): \n",
    "        return torch.mean(face_mask * F.mse_loss(original_img, reconstructed_img, reduction=\"none\"))\n",
    "    \n",
    "    def loss_structure (self, original_img, reconstructed_img, structure_mask):\n",
    "        return torch.mean(structure_mask * self.laplacian(original_img, reconstructed_img))\n",
    "    \n",
    "    def loss_texture (self, original_img, reconstructed_img, texture_mask): \n",
    "        replaced_reconstruction = self.real_value_replacement(original_img, reconstructed_img, texture_mask)\n",
    "        loss = self.args.alpha * torch.mean(texture_mask * F.l1_loss(original_img, reconstructed_img, reduction=\"none\")) + self.args.beta * self.lpips(original_img, replaced_reconstruction) + self.args.delta * self.gan_loss(original_img, replaced_reconstruction, \"generator_loss\")\n",
    "        return loss\n",
    "\n",
    "    def lpips(self, original_img, reconstructed_img):\n",
    "        LPIPS_loss = self.vgg_perceptual_loss.forward(original_img, reconstructed_img, normalize=True)\n",
    "        return torch.mean(LPIPS_loss)\n",
    "    \n",
    "    def discriminator_forward(self, orig_imgs, recon_imgs, train_generator):\n",
    "        \"\"\" Train on gen/real batches simultaneously. \"\"\"\n",
    "        # Alternate between training discriminator and compression models\n",
    "        if train_generator is False:\n",
    "            recon_imgs = recon_imgs.detach()\n",
    "\n",
    "        D_in = torch.cat([orig_imgs, recon_imgs], dim=0)\n",
    "\n",
    "        D_out = self.Discriminator(D_in)\n",
    "        D_out = torch.squeeze(D_out)\n",
    "        D_real, D_gen = torch.chunk(D_out, 2, dim=0)\n",
    "        return D_out\n",
    "\n",
    "    def gan_loss(self, original_img, replaced_reconstruction, intermediates, mode, training, writeout, step_counter):\n",
    "        disc_out = self.discriminator_forward(original_img, replaced_reconstruction, intermediates, train_generator=(mode==\"generator_loss\"))\n",
    "        D_loss, G_loss = gan_loss(gan_loss_type=self.args.gan_loss_type, disc_out=disc_out, mode='generator_discriminator_loss')\n",
    "        if mode == 'generator_discriminator_loss':\n",
    "            loss = [D_loss, G_loss]\n",
    "        elif mode == 'generator_loss':\n",
    "            loss = G_loss \n",
    "        else:\n",
    "            loss = D_loss\n",
    "        return loss\n",
    "\n",
    "    def real_value_replacement(self, original_img, reconstructed_img, texture_mask):\n",
    "        return (1-texture_mask) * original_img + texture_mask * reconstructed_img\n",
    "\n",
    "    def process_masks(self, face_mask, structure_mask):\n",
    "        structure_mask = ~face_mask & structure_mask\n",
    "        texture_mask = torch.ones_like(face_mask) & ~face_mask & ~structure_mask\n",
    "        return face_mask, structure_mask, texture_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__() \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),            # Flatten the input\n",
    "            nn.Linear(256*256*3, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 256*256*3),\n",
    "            nn.Sigmoid()  # Sigmoid activation for pixel values (assuming image in [0, 1] range)\n",
    "        )\n",
    "        # Discriminator layers\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*256*3, 256),\n",
    "            nn.Sigmoid()  # Sigmoid activation for pixel values (assuming image in [0, 1] range)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_encoded = self.encoder(x)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "        return x_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device\n",
    "dataset = ContentOrientedDataset(root='', crop_size=256, normalize=False)\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "model = Model()\n",
    "loss = ContentOrientedLoss(args, discriminator=args.discriminator)\n",
    "optimizer_encoder_decoder = torch.optim.Adam([Model.encoder.parameters(), Model.decoder.parameters()], lr=args.learning_rate)\n",
    "optimizer_discriminator = torch.optim.Adam([Model.discriminator.parameters()], lr=args.learning_rate)\n",
    "phase = \"ED\"\n",
    "for epoch in range(num_epochs):\n",
    "    for [orig_imgs, face_masks, structure_masks], bpp in dataloader:\n",
    "        recon_imgs = model(orig_imgs)\n",
    "        compression_loss, D_loss = loss(orig_imgs, recon_imgs, face_masks, structure_masks)\n",
    "        if phase==\"ED\":\n",
    "            compression_loss.backward()\n",
    "            optimizer_encoder_decoder.step()\n",
    "            optimizer_encoder_decoder.zero_grad()\n",
    "            phase = \"D\"\n",
    "        else: \n",
    "            D_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            phase = \"ED\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
